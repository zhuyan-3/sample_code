---
title: "Make daily survey" 
author: "Yan Zhu"
date: "`r lubridate::today()`"
format: 
  html:
    timeout: 180
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---   

## Notes

This script builds a final daily survey df from the combined raw survey.  It should only be run after updating `combined_daily_survey.csv` with `2_make_raw_surveys.qmd`

*user_answer_guid is the table identifier that shows what answer the user selected for a specific survey.*

## Set up
```{r Set up, echo=FALSE}
options(conflicts.policy = "depends.ok")
library(tidyverse) 
library(beepr)
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

theme_set(theme_classic())
options(dplyr.print_max=Inf)

path_processed <- format_path("studydata/risk2/data_processed/shared")
```

## Make initial daily df 
Read raw daily survey from data_tables folder.
```{r reading data}
raw <- read_csv(here::here(path_processed, "data_tables", "combined_daily.csv")) %>%  
  # filter(!str_detect(variable_name, "lapse")) %>%    # - Remove lapse and lapse_date rows
  glimpse(show_col_types = FALSE)

# making noise when this chunck finished running
beep()
```

Here did some very first cleaning.
```{r 1st change}
# save the raw data into a new df, and do rename.
daily <- raw %>% 
  rename(utc_offset = other_data) # rename other_data 
  
# change "subid" from number type to character type
daily$subid <- as.character(daily$subid)

# Change to Date time
# We lost millisecond here
daily$start_date <- as.POSIXct(daily$start_date, origin = "1970-01-01", tz = "UTC")
daily$complete_date <- as.POSIXct(daily$complete_date, origin = "1970-01-01", tz = "UTC")
daily$answer_date <- as.POSIXct(daily$answer_date, origin = "1970-01-01", tz = "UTC")

# Let’s convert the utc_offset variable into hours.
daily <- daily %>%
  mutate(utc_offset = str_extract(utc_offset, "-?\\d+"))

daily$utc_offset <- as.numeric(daily$utc_offset) # Convert utc_offset to numeric type (in minutes)
daily$utc_offset <- daily$utc_offset/60 # Convert in hours

# have a quick look on data
daily %>% glimpse(show_col_types = FALSE)
```

**Next step:** Check unnecessary variables before deleting

## Delete Columns
### Deletion 1
Here delete unnecessary columns which are have one unique variables.
```{r Find columns with only one unique value}
# Checks each column in the 'raw' data frame to determine if it contains only a single unique value
single_valued_columns <- sapply(raw, function(col) length(unique(col)) == 1)
cat("Variables with only one unique value:\n", names(raw[single_valued_columns]))
```

```{r, results='hide'}
tab(daily, survey_slug) # daily-survey
tab(daily, schedule_date) # -1
tab(daily, display_conditions) # ""
tab(daily, loop_node) # NA
tab(daily, max_number_selected_dates) # NA
tab(daily, lapse_start_date) # NA
tab(daily, lapse_start_time) # NA
tab(daily, num_hours_to_show) # NA
tab(daily, picker_format) # NA
tab(daily, multiple) # NA
tab(daily, min_value) # NA
tab(daily, max_value) # NA
tab(daily, subquestion_guid) # NA
```


There are 17 columns are deleted here because they have unique item, which is not helpful.
```{r First deletion}
# First deletion
daily <- daily %>% 
  select(-survey_slug, -schedule_date, -display_conditions, -max_date, -min_date, -loop_node, -allow_future_dates, -max_number_selected_dates, -lapse_start_date, -lapse_start_time, -num_hours_to_show, -picker_format, -multiple, -min_value, -max_value, -subquestion_guid, -answer_child_node_guid) %>% 
  glimpse(show_col_types = FALSE)
```

### Deletion 2
After deleting some columns with **only one unique item**, I continued checking if there are any columns containing items with the same meaning but in different formats across different columns.
```{r column with one item}
# Find columns with the same number of unique values and record the number of unique values
unique_counts <- sapply(daily, function(x) length(unique(x)))
unique_count_list <- split(names(unique_counts), unique_counts)

# Output columns with the same number of unique values and their number of unique values
for (key in names(unique_count_list)) {
  cat("Unique value count:", key, "\n")
  cat("Columns:", paste(unique_count_list[[key]], collapse = ", "), "\n")
}
```

In the above output, I will focus on observing variables with the same number of Unique value count and delete the redundant columns.


#### Unique value count: 2 
```{r check variables 1}
# Unique value count: 2 
# Columns: status, type 

# Unique value count: 3 
# Columns: question_type 

daily %>% select(status, question_type, type) %>% head(10)
```

Here, delete one column named 'type'. The rationale is that another column named 'question_type' contains a more comprehensive record of the question types.
```{r delete type}
daily <- daily %>%
  select(-type)
```


#### Unique value count: 5
```{r check 2}
# Unique value count: 5 
# Columns: node_guid, node_title, node_description, node_order 
daily %>% select(node_guid, node_title, node_description, child_node_guid, node_order) %>% head(10)
```

Here, I delete 4 columns. In columns where the count of unique values is equal to 3, they represent the same item. I will keep the node_description column, as it provides the most information.
```{r delete 2}
daily <- daily %>%
  select(-node_guid, -node_title, -child_node_guid, -node_order)
```


#### Unique value count: 17 
```{r check variable 3}
# Unique value count: 17 
# Columns: question_guid, question_content, question_other_data, variable_name 
daily %>% select(question_guid, question_content, question_other_data, variable_name) %>% head(10)
```

All of them represent the same item. Here, I delete two columns, keeping only 'variable_name, and question_content' which is clear and concise!
```{r delete variable 4}
daily <- daily %>%
  select(-question_guid, -question_other_data)
```


#### Unique value count: 69027 
Here, user_survey_guid was not deleted. Although the start date can be used to check the date and to distinguish between surveys, user_survey_guid's format is easier to use in some cases.
```{r check variable 4}
# Unique value count: 70654 
# Columns: user_survey_guid, start_date 
daily %>% select(user_survey_guid, start_date) %>% head(10)
```

Double check if there any unnecessary columns!!! (Using previous code)
```{r check unnecessary columns}
# Find columns with the same number of unique values and record the number of unique values
unique_counts <- sapply(daily, function(x) length(unique(x)))
unique_count_list <- split(names(unique_counts), unique_counts)

# Output columns with the same number of unique values and their number of unique values
for (key in names(unique_count_list)) {
  cat("Unique value count:", key, "\n")
  cat("Columns:", paste(unique_count_list[[key]], collapse = ", "), "\n")
}

### and keep my workplace clean.
rm(unique_counts)
rm(unique_count_list)
```

### Deletion 3

**order, answer_guid, user_answer_guid** are the last three columns that I am not clear about.
```{r Checking NA}
# Checking NA
cat("NA count for order:", sum(is.na(daily$order)), "\nNA count for answer_guid:", sum(is.na(daily$answer_guid)), "\nNA count for user_answer_guid", sum(is.na(daily$user_answer_guid)))
```

**Note:** *order* and *answer_guid* have same number of NA

Then let's check the missing value in *order* and *answer_guid* are in the same row.
```{r order or answer_guid is NA}
# get row number of order or answer_guid is NA
not_na_unknown_rows <- daily[is.na(daily$answer_guid) | is.na(daily$order), ]

paste("order or answer_guid are missing value, n rows =", nrow(not_na_unknown_rows))
```

```{r get row number of order and answer_guid}
# get row number of order and answer_guid
not_na_unknown_rows <- daily[is.na(daily$answer_guid) & is.na(daily$order), ]

paste("order and answer_guid are missing value, n rows =", nrow(not_na_unknown_rows))
```

**Great!** Those row number are the same, which means these two variable are closely related with each other.

And go head to check the *node_description* of those (order and answer_guid is NA)rows.
```{r}
table(not_na_unknown_rows$node_description)

# to keep my workplace clean.
rm(not_na_unknown_rows)
```

**Now** we are sure about the variable *oder* is the numeric number for ranking of the answers in radio type of questions.

Here delete *answer_guid*
```{r delete answer guid}
daily <- daily %>%
  select(-answer_guid)
```

```{r}
glimpse(daily)
```


## ID Check
```{r}
cat("There are", sum(is.na(daily$subid)), "subid is NA, and there are", sum(is.na(daily$user_answer_guid)), "user_answer_guid is NA. Therefore, next step is to figure out why, and use subid for analysis.")

```

**Empty string** found in user_answer_guid
```{r}
# Select rows from the daily data frame where the value of the daily$user_answer_guid column is an empty string.
na_user_answer_guid <- daily[daily$user_answer_guid == "", ] %>% glimpse()
```

Remove rows from the rows from daily data frame where the value of the daily$user_answer_guid column is an empty string.
```{r Remove rows}
daily <- subset(daily, daily$user_answer_guid != "")
cat("There are", nrow(na_user_answer_guid), "rows are removed.")

# remove df after checking and deleting
rm(na_user_answer_guid)
```

We need to select a usable ID. In theory, both subid and user_guid are usable, but since their quantities are different, we need to identify the issue.
```{r}
paste("We have", length(unique(daily$subid)), "unique subids and", 
      length(unique(daily$user_guid)), "unique user_guids.")
```

Unclear about why **subid** and **user_guid** are having different unique value count.

I assume that there are multiple user_guids associated with one subid, with two or more user_guids.
```{r ID Check}
# find out which subid had 2 user_guid
id_df <- daily %>%
  group_by(subid) %>%
  summarize(num_user_guid = n_distinct(user_guid))

id_df %>%
  filter(num_user_guid == "2")
```

And indeed, there some subid with 2 user_guids.

After talking with Susan, she explained: "We have had 4 people who for whatever reason got locked out of their account (and didn't have access to their setup email address any more) so we ended up having to create a new account for them. They ended up with two different user_guids but one subid. They are 1163, 1238, 1306, 1329."

Based on Susan's answer, I found that 1163 was not in the daily survey and gave feedback.

**Solved**  -> Susan said: "It is unexpected that 1163 is not in the daily survey at all. I can see they were paid for some over a year ago - I will look at the dataframes now to see if I can understand the issue!"

```{r more than one user_guid}
# print out all those user_guid of "1238", "1306", "1329"
daily %>%
  filter(subid %in% c("1163", "1238", "1306", "1329")) %>%
  select(user_guid) %>%
  distinct()

# delete the dataframe which I would not use later.
rm(id_df)
```

## Date Check
```{r Date Check 1}
daily %>% 
  select(subid, start_date, answer_date, complete_date, utc_offset) %>% head(5)
```

Then ensure that there are no issues with the range of dates.
```{r Checking the earliest survey}
# Checking the earliest survey
daily %>%
  arrange(start_date,) %>% 
  select(subid, user_survey_guid, start_date) %>% 
  arrange(start_date) %>% 
  glimpse()
```

Official start date was *16 April 2021*.

The earliest survey start date does not make sense: *2003-04-21 20:54:28*

```{r}
early_survey <- subset(daily, start_date <= as.POSIXct("2021-04-16 00:00:01"))
paste("Suvery ID (which take survey early than 16 April 2021:", unique(early_survey$user_survey_guid))

# clean my workplace
rm(early_survey)
```

**Note:** If not showing anymore, it is because the technology department also did some work on this date issue.

So, Delete rows before 16 April 2021.
```{r delete rows}
# Delete rows before 16 April 2021.
daily <- subset(daily, start_date >= as.POSIXct("2021-04-16 00:00:01"))

daily %>%
  arrange(start_date) %>%
  select(subid, user_survey_guid, start_date) %>%
  glimpse()
```
Take a look on completed date
```{r completed time}
# Checking the completed date of earliest survey
daily %>%
  arrange(complete_date,) %>% 
  select(complete_date) %>% 
  glimpse()
```

*"1969-12-31 23:59:59"* is automatically filled in and needs to be changed to NA.


```{r}
# any NA?
anyNA(daily$complete_date)
```

```{r 1969-12-31 23:59:59 to NA}
# Replace dates equal to "1969-12-31" in the complete_date column with NA
daily$complete_date <- ifelse(as_date(daily$complete_date) == as_date("1969-12-31"), NA, daily$complete_date)

# The ifelse() function returns a vector in which the date and time are converted to numeric values. Therefore, the data type of daily$complete_date becomes double.
daily$complete_date <- as.POSIXct(daily$complete_date, origin="1970-01-01", tz="UTC")

daily %>%
  arrange(complete_date) %>% 
  select(complete_date) %>% 
  glimpse()
```

*Question:* How many NA in complete date?
```{r}
sum(is.na(daily$complete_date))
```

And check if the count of NA value in complete_date matched with the incomplete status.
```{r}
tab(daily, status)
```

Each survey should have one unique start_date and one complete_date
```{r Make sure each start_date with one complete_date}
# Make sure each start_date with one complete_date
start_data_df <- daily %>%
  filter(!is.na(complete_date)) %>%
  group_by(start_date) %>%
  summarize(num_complete_date = n_distinct(complete_date))

unique(start_data_df$num_complete_date)

# remove the df after checking
rm(start_data_df)
```

*Checked each start date only with one the complete date which is good!*


## Duplicated rows

**UPDATE:** After duscusion in Slack, Susan had rebuilt the surveys with a fix in place for these duplicate rows in the contact table.

Check if there are now non-distinct rows introduced by removing the above columns
```{r Duplicated rows}
# Extract duplicate elements
# Each rows are labeled, if the row is duplicated with other, it will be labeled "TRUE"
dr <- duplicated(daily) | duplicated(daily, fromLast = TRUE)
table(dr)
```


```{r save duplicated rows in dataframe}
# save duplicated rows in dataframe
dr_df <- daily[dr, ]
cat("There are", nrow(dr_df), "duplicated rows in daily survey data before removing any other column or variables.")

# cat(unique(dr_df$subid), "who is/are having duplicated rows.")
### remove the df and value after checking
rm(dr_df)
rm(dr)
```

**Now, there are no duplicated case!!!!!**

**Information from Susan:** I looked back over our issue logs and found a different user having this same issue, which we discussed with our technician over at CHESS. He explained that if the user is experiencing a poor connection when the time the app tries to upload a completed survey, this could happen.

**And Now we are good to go for next step.**

The codes below are I tried to solve it by deleting those duplicated rows. And those codes are **Not helpful** for further steps.

```{r}
# # target id: 1069's duplicated rows
# target_1069 <- daily$subid == 1069
# 
# # Mark duplicate rows of target
# target_first_occurrence <- !duplicated(daily)
# 
# # Remove duplicate rows that are not the first occurrence of target id
# cleaned_1069 <- daily[!target_1069 | target_first_occurrence, ]
# 
# cat("Participant 1069 had", nrow(daily) - nrow(cleaned_1069), "duplicate rows, and they were removed.")
```


```{r}
# #And check again is there any duplicated rows.
# check_dupli <- duplicated(cleaned_1069) | duplicated(cleaned_1069, fromLast = TRUE)
# table(check_dupli)
```

## Completion

### Completed Survey

**Question:** How many completed survey? And is there any missing data?
```{r}
data_comp <- daily[daily$status == "complete", ]
cat("There are", length(unique(data_comp$user_survey_guid)), "completed surveys.
    And there are", sum(is.na(data_comp$answer)), "missing answers in those surveys.")
```

As we know:

- If an answer was not displayed (for example - lapse_date is not displayed 

- If lapse is answered "no") OR if the question was skipped, it should have NA value in the answer column.

- If the user only answered the first question(s), no additional entries are made in the surveys_user_answers table, so what you're seeing there is also correct.

```{r}
na_comp <- data_comp[is.na(data_comp$answer), ]
tab(na_comp, variable_name)

### clean my work place.
rm(na_comp)
```


### Incompleted Survey
**Question:** How many incompleted survey are recorded for now?
```{r}
data_incomp <- daily[daily$status == "incomplete", ]
cat("There are", length(unique(data_incomp$user_survey_guid)), "incompleted surveys.
    And there are", sum(is.na(data_incomp$answer)), "missing answers in those surveys.")
```

**Question:** How many participants have a history of not completing the survey？
```{r}
cat("There are", length(unique(data_incomp$subid)), "participants have a history of not completing the survey.")
```

**Question:** Who are those participants have a history of not completing the survey?
```{r}
cat("They are:", unique(data_incomp$subid))
```

**Question:** Who were more likely to not complete the survey?
```{r multiple_incomp_srvy}
multiple_incomp_srvy <- data_incomp %>%
  group_by(subid) %>%
  summarize(survey_count = n_distinct(user_survey_guid))

# Use cat() to print the survey_count of each id
multiple_incomp_srvy %>%
  arrange(desc(survey_count)) %>%
  filter(survey_count > 1) %>%
  apply(1, function(row) cat("subid:", row["subid"], "had", row["survey_count"], "incompleted surveys.", "\n"))
```

*NULL is because the apply() function returns a value after applying the given function, and the cat() function itself does not return a value, so apply() returns NULL. You can ignore this NULL.*

Check how many times each person has filled out the questionnaire among those who have not completed the questionnaire.
```{r}
incomp_hist <- daily[daily$subid %in% data_incomp$subid, ]

# Get the unique user_survey_guid corresponding to each subid
incomp_hist <- incomp_hist %>%
  distinct(subid, user_survey_guid) %>%
  group_by(subid) %>%
  summarise(unique_survey_count = n()) %>%
  arrange(subid)

cat("The range of total survey count for each participant is", range(incomp_hist$unique_survey_count))
```

```{r}
cat("The highest number of people who failed to complete the survey was recorded", max(multiple_incomp_srvy$survey_count), "times. \nThe total number of completed surveys significantly exceeds the number of incomplete surveys. \nTherefore, it is not possible to accurately identify untrustworthy participants.")

# remove after useing it
rm(incomp_hist)
rm(multiple_incomp_srvy)
```


## Survey Duration
```{r each_survey}
data_comp <- daily[daily$status == "complete", ]

each_survey <- data_comp %>%  
  group_by(subid, user_survey_guid, start_date, complete_date) %>% 
  summarize(question_count = n(), .groups = "drop")

# calculate the survey duration in seconds unit
each_survey$survey_duration_sec <- each_survey$complete_date - each_survey$start_date
each_survey$survey_duration_sec <- as.numeric(each_survey$survey_duration_sec)

# calculate the survey duration in minutes unit
each_survey$survey_duration_min <- each_survey$survey_duration_sec / 60
each_survey$survey_duration_min <- as.numeric(each_survey$survey_duration_min)
each_survey$survey_duration_min <- round(each_survey$survey_duration_min, 2)

each_survey %>% glimpse()
```

Let's plot to check the histogram of survey duration
```{r, warning=FALSE}
ggplot(each_survey, aes(x = survey_duration_min)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  ggtitle(paste("Survey Duration Histogram", "\nSample Size:", nrow(each_survey))) +
  ylab("Frequency") +
  xlab("Duration in Minute")
```

```{r}
cat("With", nrow(each_survey), "survey, the mean of duration is", mean(each_survey$survey_duration_min), "minutes.")
```

Therefore, I would like to remove the survey which took over 15 minutes.
```{r}
each_survey <- each_survey[each_survey$survey_duration_min < 15, ]

ggplot(each_survey, aes(x = survey_duration_min)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  ggtitle(paste("Survey Duration Histogram", "\nSample Size:", nrow(each_survey))) +
  ylab("Frequency") +
  xlab("Duration in Minute")

# # deleting in the daily
# daily <- daily %>%
#   filter(user_survey_guid %in% each_survey$user_survey_guid)
```


**Z Score Method** to check the outliers
```{r}
# Calculate Z-scores
each_survey <- each_survey %>%
  group_by(subid) %>%
  mutate(z_scores = scale(survey_duration_min))

# Find outliers
outliers_z <- each_survey[abs(each_survey$z_scores) > 3, ]

ggplot(outliers_z, aes(x = survey_duration_min)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  ggtitle(paste("Survey Duration Histogram(outliers by Z scores)", "\nSample Size:", nrow(outliers_z))) +
  ylab("Frequency") +
  xlab("Duration in Minute")
```

**IQR** to check the outliers
```{r}
# Group by id (subid) and calculate quartiles
quartiles <- each_survey %>%
  group_by(subid) %>%
  summarize(q1 = quantile(survey_duration_min, 0.25),
            q3 = quantile(survey_duration_min, 0.75),
            iqr = q3 - q1)

each_survey <- merge(each_survey, quartiles, by = "subid")
## keep my workplace clean
rm(quartiles)

# Define outlier threshold
each_survey$lower_threshold <- each_survey$q1 - 1.5 * each_survey$iqr
each_survey$upper_threshold <- each_survey$q3 + 1.5 * each_survey$iqr

# Identify outliers based on thresholds
outliers_iqr <- each_survey %>%
  filter(survey_duration_min < lower_threshold | survey_duration_min > upper_threshold)

ggplot(outliers_iqr, aes(x = survey_duration_min)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  ggtitle(paste("Survey Duration Histogram(outliers by IQR)", "\nSample Size:", nrow(outliers_iqr))) +
  ylab("Frequency") +
  xlab("Duration in Minute")
```

## Number of Question
**Question:** How many questions are a single survey?
```{r How many questions are a single survey?}
cat("We have", length(unique(daily$variable_name)), "question in a survey.")
```

**Question:** Did everyone answered all 17 question in completed survey? 
```{r}
tab(each_survey, question_count)
```

```{r}
ggplot(each_survey, aes(x = question_count)) +
  geom_bar(fill = "skyblue", color = "black") +
  geom_text(stat = "count", aes(label = paste(after_stat(count))), vjust = -0.5) +
  labs(x = "Number of Questions",
       y = "Frequency",
       title = "Histogram of Question Count in each Survey")
```

**Notice!:** This issue still need more discussion.
```{r}
more_q <- each_survey[each_survey$question_count > 17, ]

cat(names(table(more_q$subid)), "had", table(more_q$subid), "completed survey with more than", length(unique(daily$variable_name)), "questions.
    And survey ID is", unique(more_q$user_survey_guid))
```

**Question:** Which question were have answered more than one times?
```{r extra_ans}
# Using user_survey_guid tack the survey records.
extra_ans <- daily[daily$user_survey_guid %in% more_q$user_survey_guid, ]
tab(extra_ans, variable_name)
```

**Question:** How many duplicated questions are recorded?
```{r}
# find out the rows with question were answered more than one times.
extra_ans_dup <- extra_ans %>%
  count(user_survey_guid, variable_name, name = "num_answer") %>% 
  glimpse()

cat("There are",sum(extra_ans_dup$num_answer == 2), "questions had answered twice in", length(unique(extra_ans_dup$user_survey_guid)), "surveys.")
```


```{r matched_rows}
extra_ans_filtered <- extra_ans_dup[extra_ans_dup$num_answer == 2, ]

# Initialize an empty data frame to store rows that meet the conditions
matched_rows <- data.frame(user_survey_guid = character(),
                           variable_name = character(),
                           num_answer = integer())

# Loop through each row in extra_ans_filtered
for (i in 1:nrow(extra_ans_filtered)) {
  # Get the user_survey_guid and variable_name of the current row
  current_user_survey_guid <- extra_ans_filtered$user_survey_guid[i]
  current_variable_name <- extra_ans_filtered$variable_name[i]
  
  # Find matching rows in daily
  matched_row <- daily[daily$user_survey_guid == current_user_survey_guid & 
                         daily$variable_name == current_variable_name, ]
  
  # If a matching row is found, add it to matched_rows
  if (nrow(matched_row) > 0) {
    matched_rows <- rbind(matched_rows, matched_row)
  }
}

matched_rows <- matched_rows %>% select(user_survey_guid, answer_date, answer, question_type, variable_name, everything())

matched_rows
```

**Update:** Susan Wanta: "@Yan, I think this is due to the server connection issue, and I'm working on a way to filter these out before we save the daily_combined.csv dataframe. I'll let you know when I have that fix in place!"

## Duplicated Survey
Make a new variable called "subid_day".
```{r}
# Step 1: Extract the date part
each_survey <- each_survey %>% 
  arrange(subid, start_date) %>% 
  mutate(survey_date = as.Date(start_date))

daily <- daily %>% 
  arrange(subid, start_date) %>% 
  mutate(survey_date = as.Date(start_date))

# Step 2: Make subid_day
each_survey$subid_day <- paste(each_survey$subid, each_survey$survey_date, sep = "_")
daily$subid_day <- paste(daily$subid, daily$survey_date, sep = "_")

# Step 3: remove survey_date.
each_survey <- each_survey %>% 
  select(-survey_date)

daily <- daily %>% 
  select(-survey_date)
```


```{r}
# Filter out the combinations that occur more than once
du_check <- each_survey %>%
  group_by(subid_day) %>%
  mutate(repeat_count = n()) %>%
  ungroup()

# Filter out duplicated survey
du_check <- du_check[du_check$repeat_count > 1, ]
tab(du_check, repeat_count)
```

### 7 Survey/Day
**Check case with 7 survey a day.**
```{r}
# 7	7	0.0006266786	
check1 <- du_check[du_check$repeat_count == 7, ]

cat("subid", unique(check1$subid), 
    "had", length(unique(check1$user_survey_guid)), "surveys with", 
    unique(check1$question_count), "questions in a day.\n",
    "Time range:", format(range(check1$start_date)[1]), "-", format(range(check1$start_date)[2]), "\n")
```

```{r}
check1 <- daily[daily$user_survey_guid %in% check1$user_survey_guid, ]

tab(check1, variable_name) # all 7 survey with 16 questions.
rm(check1)
```

### 5 Survey/Day
```{r}
# 5	10	0.0008952551		
check2 <- du_check[du_check$repeat_count == 5, ]

cat("There are", length(unique(check2$subid_day)), "people answered", unique(check2$repeat_count), "times with", unique(check2$question_count), "question in a day.")

check2 <- daily[daily$user_survey_guid %in% check2$user_survey_guid, ]

# tab(check2, variable_name)

rm(check2)
```

### 4 Survey/Day
```{r}
# 4	28	0.0025067144		
check3 <- du_check[du_check$repeat_count == 4, ]

cat("There are", length(unique(check3$subid_day)), "people answered", unique(check3$repeat_count), "times with", unique(check3$question_count), "question in a day.")

check3 <- daily[daily$user_survey_guid %in% check3$user_survey_guid, ]

# tab(check3, variable_name)

rm(check3)
```

### 3 Survey/Day
```{r}
# 3	159	0.0142345568	
check4 <- du_check[du_check$repeat_count == 3, ]

cat("There are", length(unique(check4$subid_day)), "people answered", unique(check4$repeat_count), "times with", unique(check4$question_count), "question in a day.")

check4 <- daily[daily$user_survey_guid %in% check4$user_survey_guid, ]

# tab(check4, variable_name)

rm(check4)
```

### 2 Survey/Day
```{r}
# 2	10966	0.9817367950	
check5 <- du_check[du_check$repeat_count == 2, ]

cat("There are", length(unique(check5$subid_day)), "people answered", unique(check5$repeat_count), "times with", unique(check5$question_count), "question in a day.")

check5 <- daily[daily$user_survey_guid %in% check5$user_survey_guid, ]

# tab(check5, variable_name)

rm(check4)
```



## EDA
Let's do the EAD after we deleted duplicated survey and survey with abnormal survey duration.

### Basic

**Question:** How many participants?
```{r How many participants?}
cat("We have", length(unique(daily$subid)), "participants for now.")
```

**Question:** How many completed survey are recorded for now?
```{r How many survey are recorded for now?}
paste("There are ", length(unique(data_comp$user_survey_guid)), "surveys in total for now.")
```





## Draft (Exploring continue)
**Question:** Why slightly different counts for each question?
```{r}
table(daily$variable_name)
```

Let's get how many times each person answered different types of questions
```{r}
# Create a DF to record how many times each question was answered
variable_count <- daily %>%
  group_by(subid, variable_name) %>%
  summarize(count = n()) %>%
  pivot_wider(names_from = variable_name, values_from = count, values_fill = 0)

glimpse(variable_count)
```

Find out people who had different count in the total number of times answering each question.
```{r}
# I assume the value of each variable in adherence to surge is in the corresponding column. If each variable is equal to the value of adherence, the value of the new column variable_consistency is 0, otherwise it is 1.

variable_count <- variable_count %>%
  mutate(variable_consistency = ifelse(
    adherence == angry & 
    adherence == anxious &
    adherence == confidence &
    adherence == depressed &
    adherence == drugs &
    adherence == happy &
    adherence == hassle &
    adherence == motivation &
    adherence == pain &
    adherence == pleasant &
    adherence == relaxed &
    adherence == risk &
    adherence == sleep &
    adherence == urge, 0, 1
  ))

# filter out the count for each question are not having same number of count.
not_consist <- filter(variable_count, variable_consistency == 1) %>% glimpse()
```

```{r}
unique(not_consist$subid)
```


**Question:** Could incomplete status be influenced by travel status?
```{r different timezone}
### count how many timezone are recorded for each person
offset_hours_count <- data_incomp %>%
  group_by(subid) %>%
  summarize(unique_offset_hours = n_distinct(utc_offset))

ggplot(offset_hours_count, aes(x = factor(unique_offset_hours))) +
  geom_bar(fill = "lightblue") +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5) +
  labs(x = "Unique Offset Hours", y = "Frequency", title = "Distribution of Unique Offset Hours") +
  theme_bw()
```


3. Finding out the outlier of average duration time in the whole sample size
```{r}
# merged_df <- left_join(complete_ans, each_survey, by = c("subid", "start_date", "complete_date"))
# merged_df <- merge(merged_df, sum_time_avg, by = "subid", all.x = TRUE)
# 
# # Filter based on avg_time_quantile_25 and avg_time_quantile_75
# rm_outlier <- merged_df %>%
#   filter(ans_avg_time >= avg_time_quantile_25 & ans_avg_time <= avg_time_quantile_75)
# 
# rm_outlier <- rm_outlier %>%
#   select(subid, ans_avg_time, avg_time_quantile_25, avg_time_quantile_75, everything()) %>%
#   glimpse()
```


```{r}
# rm_outlier_sum <- rm_outlier  %>% 
#   group_by(subid, start_date, complete_date) %>% 
#   summarize(question_count = n(), .groups = "drop")
# 
# rm_outlier_sum$survey_duration <- rm_outlier_sum$complete_date - rm_outlier_sum$start_date
# rm_outlier_sum$survey_duration <- as.numeric(rm_outlier_sum$survey_duration)
# 
# rm_outlier_sum$ans_avg_time <- rm_outlier_sum$question_count / rm_outlier_sum$survey_duration
# 
# 
# rm_outlier_sum <- rm_outlier_sum %>%
#   group_by(subid) %>%
#   reframe(
#     avg_time_mean = mean(ans_avg_time, na.rm = TRUE),
#     avg_time_median = median(ans_avg_time, na.rm = TRUE),
#     avg_time_sd = sd(ans_avg_time, na.rm = TRUE),
#     avg_time_min = min(ans_avg_time, na.rm = TRUE),
#     avg_time_max = max(ans_avg_time, na.rm = TRUE),
#     avg_time_quantile_25 = quantile(ans_avg_time, na.rm = TRUE, probs = 0.25),
#     avg_time_quantile_75 = quantile(ans_avg_time, na.rm = TRUE, probs = 0.75)
#   )
```

```{r}
# head(rm_outlier_sum, 6)
```



```{r}
# histogram1 <- ggplot(sum_time_avg, aes(x = avg_time_mean, fill = "Before Removed")) +
#   geom_histogram(alpha = 0.5) +
#   labs(title = "Histogram of avg_time_mean") +
#   theme_minimal()
# 
# plot_overlay <- histogram1 +
#   geom_histogram(data = rm_outlier_sum, aes(fill = "Outlier Removed"), alpha = 0.5　) +
#   scale_fill_manual(values = c("blue", "red"), guide = FALSE) +  
#   labs(fill = "Dataset") +  # 图例标题
#   guides(fill = guide_legend(title = "Dataset")) 
# 
# plot_overlay　
```


```{r}
# cat("Rows in completed and no duplicated:", nrow(merged_df), "\n")
# cat("Rows in after remove the outlier base on each average durtion:", nrow(rm_outlier), "\n")
```


### Save Daily Surveys
```{r save_daily}
# daily |> write_csv(here::here(path_processed, "survey_daily.csv"))
```
