---
title: "Initial Cleaning and Data Anonymization"
author: "Yan Zhu"
date: "`r lubridate::today()`"
format: 
  html:
    timeout: 180
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---
## Matching IDs and Data Anonymization

This code file is used for matching IDs and data anonymization to ensure data privacy. The code includes steps for processing data, matching IDs, and implementing data anonymization measures to ensure the security of sensitive information.

**Note:** The output of the data might be hidden due to it displaying personal information.

### Data Processing Steps

- Setup package and read the data file
- Execute data matching
- Perform data anonymization
- Do some initial cleaning
- Delete unnecessary data
- Generate the final result


## Setup
```{r setup, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
source("/Users/yan/Desktop/sample_code/functions.R")

library(tidyverse)
library(dplyr)
library(skimr)
library(janitor)
library(readxl)
```

```{r read_data, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
baseline <- read_csv("/Users/yan/Desktop/sample_code/ESM/0_raw data/esm_bline_feb29.csv", show_col_types = FALSE) %>%
  select( -new_random_id) %>% 
  clean_names()

daily <- read_csv("/Users/yan/Desktop/sample_code/ESM/0_raw data/esm_diary_raw_data.csv", show_col_types = FALSE) %>%
  clean_names()
daily <- daily[-c(1, 2), ]

sona_id <- read_excel("/Users/yan/Desktop/sample_code/ESM/0_raw data/PID_SonaID_match.xlsx") %>%
  clean_names()
```

## Execute data matching
```{r check baseline id}
# check id column's name in baseline
grep("id", names(baseline ), value = TRUE)
```

```{r}
# rename the variable name
baseline <- baseline %>% 
  rename(sona_id = new_sona_id)

# change variable in character type
baseline$sona_id <- as.character(baseline$sona_id)
```


```{r}
# check is there any missing data in ID column
any(is.na(baseline$sona_id))
```

```{r check daily id}}
# check id column's name in daily
grep("id", names(daily), value = TRUE)
```

```{r}
# check is there any missing data in ID column
any(is.na(daily$pid))
```


There is only "PID" in the daily survey, so "PID" and "SONA ID" need to be matched.
```{r}
daily <- merge(daily, sona_id, by = "pid", all.x = TRUE)

# change variable in character type
daily$sona_id <- as.character(daily$sona_id)
```


## Data Anonymization
```{r}
cat("There are", length(unique(baseline$sona_id)), "participants in the baseline survey with", nrow(baseline), "rows, \nwhich means some participants completed the baseline survey more than once.")
```



```{r}
cat("We have", length(unique(daily$sona_id)), "participants who answered", nrow(daily), "daily surveys")
```

```{r}
# Save all the unique SONA ID in a new df
unique_sona_id <- baseline %>%
  distinct(sona_id)

# Generate a unique random number ID for each different sona_id
set.seed(56565656)  # Set a seed for reproducibility
unique_sona_id$random_id = sample(100000:999999, length(unique(baseline$sona_id)))


# Check if there are duplicate values in the random_id
any(duplicated(unique_sona_id$random_id))
```

```{r}
baseline <- merge(baseline, unique_sona_id, by = "sona_id", all.x = TRUE)

daily <- merge(daily, unique_sona_id, by = "sona_id", all.x = TRUE)
```

```{r, echo=TRUE, results='hide'}
check_baseline <- baseline[duplicated(baseline$random_id), ]
 
check_baseline %>% select(random_id, sona_id)
```

```{r, echo=TRUE, results='hide'}
check_daily <- daily[duplicated(daily$random_id), ]
 
check_daily %>% select(random_id, sona_id)
```


**Note:** Based on the check in the previous step, it was found that daily$random_id has missing data, which indicates that some people did not fill in the baseline record.

```{r}
sum(is.na(daily$random_id))
```

```{r, echo=TRUE, results='hide'}
daily_miss <- daily[is.na(daily$random_id),]

unique(daily_miss$sona_id)
```

```{r}
is_included <- unique(daily_miss$sona_id) %in% baseline$sona_id
table(is_included)
```

*There 26 participants did not have any records in Baseline survey.*

Remove all the survey which answered by those 26 participant
```{r}
daily <- daily[!is.na(daily$random_id),]
nrow(daily)
```


And now, remove the SONA ID and PID in original data
```{r}
baseline <- baseline %>%
  select(-sona_id)

daily <- daily %>%
  select(-pid, -sona_id)
```

```{r}
grep("id", names(baseline), value = TRUE)
```

```{r}
grep("id", names(daily), value = TRUE)
```

## Clean Baseline
Check is there any participants have more than one rows in Baseline data.
```{r}
# check duplicated rows by random_id.
dupli_baseline <- baseline[duplicated(baseline$random_id), ]
cat("There are", length(unique(dupli_baseline$random_id)), "participants answered basline question more than once")
```

In this case, I would like to keep the records with more answers.
```{r}
dupli_baseline$na_count <- rowSums(is.na(dupli_baseline))
dupli_baseline %>% select(random_id, na_count)
```

```{r}
# keep rows with more answers for each random ID in duplicated rows.
baseline$na_count <- rowSums(is.na(baseline))
baseline <- baseline %>%
  group_by(random_id) %>%
  filter(na_count == max(na_count)) %>%
  select(-na_count)
```

Next, I would like to do a little cleaning in Baseline data. 
```{r}
# Removing some unnecessary columns.
find_single_valued_columns(baseline)
```

```{r}
baseline %>% select(consent, sc_18, sc_au, addcode) %>% head()
```

Deleting list below:

- consent: 1 = Yes (Agree to continue with the survey.)
- sc_18: 1 = Yes (Are you at least 18 years old?)
- sc_au: 1 = Yes (Have you consumed any alcohol in the last 12 months?)
- addcode: All missing data, not helpful.

sc_18 and sc_au are 2 mandatory questions have to answer, and the answer have to be 1 to continue the survey.
```{r}
baseline <- baseline %>% select(-consent, -sc_18, -sc_au, -addcode)
```


**More cleaning:** Delete baseline data for individuals who did not participate in daily questionnaire recordings.
```{r}
baseline <- baseline[baseline$random_id %in% daily$random_id, ]
```

## Clean Daily

### delete columns
```{r}
# find out columns with only one unique value
find_single_valued_columns(daily)
```

```{r}
# Check the variables before deleting.

# daily %>% select(recipient_last_name, recipient_first_name, recipient_email, external_reference, distribution_channel, user_language, q186_8_text, psb15_10_text, psb15_tried_10_text, psb16_10_text, psb16_tried_10_text, osbb7_5_text, osbb10_11_10_text, osbb15_5_text, ssab6_7_10_text, ssid, sms, sig, r_date, time, rt, rid)
```

```{r}
# delete columns with only one unique value
daily <- daily %>% 
  select(-recipient_last_name, -recipient_first_name, -recipient_email, -external_reference, -distribution_channel, -user_language, -q186_8_text, -psb15_10_text, -psb15_tried_10_text, -psb16_10_text, -psb16_tried_10_text, -osbb7_5_text, -osbb10_11_10_text, -osbb15_5_text, -ssab6_7_10_text, -ssid, -sms, -sig, -r_date, -time, -rt, -rid, -response_id) 
```

### rename
```{r}
colnames(daily)
```

```{r}
# Renaming columns
daily <- daily %>% 
  rename(
    # Daily Screeners
    aub1 = au_db1,
    osb1 = osbb1,
    ssb1 = ssab1,
    dub1 = drugs_screen_fu,
    bpa = bpa1,
    pub1 = porn_screen_fu,
    
    # Alcohol
    au_t = au_db2,
    au_v = au_db3,
    au_l = au_db4,
    au_loc_txt = au_db4_7_text,
    au_pbs = au_db5,
    au_pbs_txt = au_db5_6_text,
    au_pg = au_db6,
    au_dg = au_db7,
    au_bib = au_db8,
    au_fib = au_db9,
    au_drg = q186,
    
    # Partnered Sexual Behavior 
    cub1 = psb3,
    cub2 = psb4,
    cub3 = psb5,
    cub4 = psb6,
    cub5 = psb8,
    cub6 = psb9,
    psb3 = psb10,
    psb4 = psb10_only_one,
    psb5 = psb11_only_one,
    psb5a = psb11_tried,
    psb6 = psb12_only_one,
    psb6a = psb12_tried,
    str_p = q211,
    psb7 = psb13,
    psb8 = q198_1,
    psb9 = psb14_only_one_1,
    psb7a = psb13_tried,
    psb8a = psb_selfdrunk_tried_1,
    psb9a = psb14_tried_1,
    psb10 = psb15,
    psb10a = psb15_tried,
    psb11 = psb16,
    psb11a = psb16_tried,
    med1_p = pbs17_2,
    med2_p = pbs17_3,
    med3_p = pbs17_4,
    med4_p = pbs17_5,
    aro_pf = psb18_to_23_1,
    pls_pf = psb18_to_23_2,
    con_pf = psb18_to_23_3,
    ver_pc = q141_1,
    enon_pc = q141_2,
    inon_pc = q141_3,
    prp_v = pbs28_31_1,
    prp_p = pbs28_31_2,
    prp_sf = pbs28_31_3,
    vic_v = psb24_to_27_1,
    vic_p = psb24_to_27_2,
    vic_sf = psb24_to_27_3,
    vic_in = psb24_to_27_4,
    exp_p = psb32_1,
    st_p = psb33_1,
    pa1_p = psb34_1_to_34_5_1,
    na1_p = psb34_1_to_34_5_2,
    na2_p = psb34_1_to_34_5_3,
    na3_p = psb34_1_to_34_5_4,
    na4_p = psb34_1_to_34_5_5,
    na5_p = psb34_1_to_34_5_6,
    pa2_p = psb34_1_to_34_5_7,
    er1_p = psb34,
    er2_p = psb35,
    er3_p = psb36,
    er4_p = psb37,
    
    # Online Sexual Behavior
    osb2 = osbb2,
    osb3 = osbb3,
    osb4 = osbb4,
    osb5 = osbb5,
    osb6 = osbb6,
    osb7 = osbb7,
    osb8 = osbb8,
    osb9 = osbb9,
    osb10 = osbb10_11,
    aro_of = osbb12_to_13_1,
    pls_of = osbb12_to_13_2,
    pss = osbb14,
    exp_o = osbb16_1,
    str_o = osbb17_1,
    pa1_o = osbb18_1_to_5_1,
    na1_o = osbb18_1_to_5_2,
    na2_o = osbb18_1_to_5_3,
    na3_o = osbb18_1_to_5_4,
    na4_o = osbb18_1_to_5_5,
    pa2_o = osbb18_1_to_5_6,
    na5_o = osbb18_1_to_5_7,
    er1_o = osbb19,
    er2_o = osbb20,
    er3_o = osbb21,
    er4_o = osbb22,
    
    # Solo Sexual Behavior
    ssb2 = ssab2,
    ssb3 = ssab3,
    ssb4 = ssab4,
    ssb5 = ssab5,
    ssb6 = ssab6_7,
    aro_sf = ssab8_to_9_1,
    pls_sf = ssab8_to_9_2,
    exp_s = ssab10_1,
    str_s = ssab11_1,
    pa1_s = ssab12_1_to_5_1,
    na1_s = ssab12_1_to_5_2,
    na2_s = ssab12_1_to_5_3,
    na3_s = ssab12_1_to_5_4,
    na4_s = ssab12_1_to_5_5,
    na5_s = ssab12_1_to_5_6,
    pa2_s = ssab12_1_to_5_7,
    er1_s = ssab13,
    er2_s = ssab14,
    er3_s = ssab15,
    er4_s = ssab16,
    
    # Drug Use Behavior
    dub2 = q215,
    dub3_bf = q202,
    dub3_du = q208,
    dub3_af = q210,
    med1_d = q212_1,
    med2_d = q212_2,
    med3_d = q212_3,
    med4_d = q212_4,
    exp_d = q195,
    str_d = q196_1,
    dub4 = q179,
    dub5 = q180,
    dub6 = q181,
    
    # General Mental Health 
    imp1 = mis1,
    imp2 = mis2,
    imp3 = mis3,
    imp4 = mis4,
    dep1 = phq8_1_fu,
    dep2 = phq8_2_fu,
    insom = phq8_3_fu,
    fatig = phq8_4_fu,
    anx1 = gad_fu_nervous,
    anx2 = gad_fu_notabletostop,
    traum1 = b1,
    traum2 = c2,
    traum3 = d2,
    traum4 = e4
  )

daily <- daily %>% 
  rename(
    # Open-text entry
    dub2_txt = q215_8_text,
    dub3_bf_txt = q202_36_text,
    dub3_du_txt = q208_35_text,
    dub3_af_txt = q210_34_text,
    str_p_txt = q211_7_text,
    osb2_txt = osbb2_13_text,
    prp_in = pbs28_31_4,
    osb6_txt = osbb6_5_text,
    ssb3_txt = ssab3_8_text
  )

names(daily)
```

### recode
```{r}
# Recode screeners for each behavioral category; 1=yes, 0=no, 3=prefer not to say
daily <- daily %>%   
  mutate(across(c(aub1:psb1, osb1:ssb1, dub1:bpa, pub1),
                ~dplyr::recode(., 
                               "1" = 1,
                               "2" = 0,
                               "3" = 3,
                               .default = NA_real_)))
# Recode values
daily <- daily %>%
  mutate(across(
    c(au_pg, au_dg, psb3, psb5, psb5a, er1_p, er2_p, 
      er3_p, er4_p, osb4, osb5, osb8, pss, er1_o, er2_o, 
      er3_o, er4_o, ssb2, er1_s, er2_s, er3_s, er4_s),
    ~ case_when(
      . == "1" ~ 1, # Yes
      . == "2" ~ 0, # No
      TRUE ~ as.numeric(.)
    )
  ))


daily <- daily %>%
  mutate(across(
    c(na1_s, na2_s, na3_s, na4_s, na5_s,
      na1_o, na2_o, na3_o, na4_o, na5_o,
      pa1_s, pa2_s, pa1_o, pa2_o),
    ~ case_when(
      . == "1" ~ 0, 
      . == "2" ~ 1, 
      . == "3" ~ 2,
      . == "4" ~ 3,
      . == "5" ~ 4,
      TRUE ~ as.numeric(.)
    )
  ))

# Specific handling for "Not Sure" and "Wasn't Needed"
daily <- daily %>%
  mutate(across(
    c(au_bib, au_fib, cub1, cub2, cub3, cub4, cub5, cub6),
    ~ case_when(
      . == "1" ~ 2, # Yes
      . == "3" ~ 1, # Not Sure
      . == "2" ~ 0, # No
      TRUE ~ as.numeric(.)
    )
  )) %>%
  mutate(across(
    c(cub1, cub2, cub4),
    ~ case_when(
      . == "4" ~ 3, # Wasn't Needed
      TRUE ~ as.numeric(.)
    )
  ))

# Recode values
daily <- daily %>%
  mutate(across(
    c(dub5, dub6),
    ~ case_when(
      . == "1" ~ 2, # Yes
      . == "3" ~ 1, # Can't remember (dub5); Maybe (dub6)
      . == "2" ~ 0, # No
      TRUE ~ as.numeric(.)
    )
  ))

# Check recoding
daily %>% select(au_pg, au_dg, au_bib, au_fib, cub1, cub2, cub3, cub4, cub5, cub6,
             psb3, psb5, psb5a, er1_p, er2_p, er3_p, er4_p, osb4, osb5, osb8, pss, er1_o, 
             er2_o, er3_o, er4_o, ssb2, er1_s, er2_s, er3_s, er4_s, dub5, dub6,
             na1_s, na2_s, na3_s, na4_s, na5_s,
             na1_o, na2_o, na3_o, na4_o, na5_o,
              pa1_s, pa2_s, pa1_o, pa2_o) %>% map(tabyl)

```

### Dupilicated daily
Before find out the dupilicated daily, deleted all the survey with 0 progress.
```{r}
# deleted all the survey with 0 progress
daily <- daily %>% 
  filter(progress != 0)
```

**Step1**: created new column “id_day”.
```{r}
# created new column “id_day”
daily$id_day <- paste(daily$random_id, daily$day, sep = "_") 
daily <- daily %>% select(random_id, id_day, recorded_date, everything())

# sorted_data
daily <- daily %>%
  group_by(random_id) %>%
  arrange(recorded_date)

# Find out the duplicated row of same participants recorded survey more than one times.
duplicate_rows <- daily[duplicated(daily$id_day) | duplicated(daily$id_day, fromLast = TRUE), ]

# sorted_data
duplicate_rows <- duplicate_rows %>%
  group_by(random_id) %>%
  arrange(id_day)

cat("There are", nrow(duplicate_rows), "duplicate rows in daily survey data.")
```

Delete duplicated rows
```{r}
# Selecting on higher value in Progress between duplicated "id_day"
# Sort the data frame by id_day and progress in descending order
daily <- daily %>%
  arrange(id_day, desc(progress))

# Keep only the first occurrence of each id_day with the highest progress
daily <- daily[!duplicated(daily$id_day), ]

# check again for duplicated rows.
any(duplicated(daily$id_day))
```

```{r}
# Count number of surveys per random_id
daily_counts <- daily %>%
  group_by(random_id) %>%
  summarise(num_surveys = n()) %>% 
  ungroup()

range(daily_counts$num_surveys)
```

**NICE!!!** 28 is the Max number of records for one participant.


## Update

1. SONA ID and PID have been excluded from data files, and random_id will be utilized in place of these previous IDs.

2. There were 594 rows(surveys) deleted from this file because there were 26 participants who answered those 594 surveys did not completed the baseline survey.

3. Deleted 3 columns.

4. Removed duplicated columns in baseline survey and daily survey.

5. Matched the ID in Baseline and Daily survey data, and deleted some rows in Baseline data for individuals who did not participate in daily survey recordings.

6. Rename the columns' name and recode the data.


## Save 
```{r}
write.csv(baseline, file = "/Users/yan/Desktop/sample_code/ESM/1_anonymization/anonymous_baseline.csv", row.names = FALSE)

write.csv(daily, file = "/Users/yan/Desktop/sample_code/ESM/1_anonymization/anonymous_daily.csv", row.names = FALSE)
```



